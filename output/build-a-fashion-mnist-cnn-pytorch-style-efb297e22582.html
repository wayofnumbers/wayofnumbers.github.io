<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="author" content="Michael Li" />

        <meta name="twitter:creator" content="@lymenlee">
        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary">

<meta name="keywords" content="Machine Learning, Artificial Intelligence, Machine Learning, " />

<meta property="og:title" content="Letâ€™s Build a Fashion-MNIST CNN, PyTorchÂ Style  - A Line-by-line guide on how to structure a PyTorch ML project from scratch using Google Colab and TensorBoard "/>
<meta property="og:url" content="https://wayofnumbers.com/build-a-fashion-mnist-cnn-pytorch-style-efb297e22582.html" />
<meta property="og:description" content="Letâ€™s Build a Fashion-MNIST CNN, PyTorchÂ Style" />
<meta property="og:site_name" content="Way of Numbers" />
<meta property="og:article:author" content="Michael Li" />
<meta property="og:article:published_time" content="2020-01-30T11:13:57-06:00" />
<meta name="twitter:title" content="Letâ€™s Build a Fashion-MNIST CNN, PyTorchÂ Style  - A Line-by-line guide on how to structure a PyTorch ML project from scratch using Google Colab and TensorBoard ">
<meta name="twitter:description" content="Letâ€™s Build a Fashion-MNIST CNN, PyTorchÂ Style">

        <title>Letâ€™s Build a Fashion-MNIST CNN, PyTorchÂ Style  - A Line-by-line guide on how to structure a PyTorch ML project from scratch using Google Colab and TensorBoard  Â· Way of Numbers
</title>
        <link href="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/css/bootstrap-combined.min.css" rel="stylesheet">
        <link href="//netdna.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.css" rel="stylesheet">
        <link rel="stylesheet" type="text/css" href="https://wayofnumbers.com/theme/css/pygments.css" media="screen">
        <link rel="stylesheet" type="text/css" href="https://wayofnumbers.com/theme/tipuesearch/tipuesearch.css" media="screen">
        <link rel="stylesheet" type="text/css" href="https://wayofnumbers.com/theme/css/elegant.css" media="screen">
        <link rel="stylesheet" type="text/css" href="https://wayofnumbers.com/theme/css/admonition.css" media="screen">
        <link rel="stylesheet" type="text/css" href="https://wayofnumbers.com/theme/css/custom.css" media="screen">

        <link rel="shortcut icon" href="https://wayofnumbers.com/theme/images/favicon.ico" type="image/x-icon" />
        <link rel="icon" href="https://wayofnumbers.com/theme/images/apple-touch-icon-152x152.png" type="image/png" />
        <link rel="apple-touch-icon" href="https://wayofnumbers.com/theme/images/apple-touch-icon.png"  type="image/png" />
        <link rel="apple-touch-icon" sizes="57x57" href="https://wayofnumbers.com/theme/images/apple-touch-icon-57x57.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="72x72" href="https://wayofnumbers.com/theme/images/apple-touch-icon-72x72.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="76x76" href="https://wayofnumbers.com/theme/images/apple-touch-icon-76x76.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="114x114" href="https://wayofnumbers.com/theme/images/apple-touch-icon-114x114.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="120x120" href="https://wayofnumbers.com/theme/images/apple-touch-icon-120x120.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="144x144" href="https://wayofnumbers.com/theme/images/apple-touch-icon-144x144.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="152x152" href="https://wayofnumbers.com/theme/images/apple-touch-icon-152x152.png" type="image/png" />
        <link href="https://wayofnumbers.com/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Way of Numbers - Full Atom Feed" />
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
     (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
     m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
     })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-148798026-1', 'auto');
    ga('send', 'pageview');
</script>


    </head>
    <body>
        <div id="content">
            <div class="navbar navbar-static-top">
                <div class="navbar-inner">
                    <div class="container-fluid">
                        <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                        </a>
                        <a class="brand" href="https://wayofnumbers.com/"><span class=site-name>Way of Numbers</span></a>
                        <div class="nav-collapse collapse">
                            <ul class="nav pull-right top-menu">
                                <li >
                                    <a href=
                                       https://wayofnumbers.com
                                    >Home</a>
                                </li>
                                <li ><a href="https://wayofnumbers.com/pages/about-me.html">About&nbsp;Me</a></li>
                                <li ><a href="https://wayofnumbers.com/categories">Categories</a></li>
                                <li ><a href="https://wayofnumbers.com/tags">Tags</a></li>
                                <li ><a href="https://wayofnumbers.com/archives">Archives</a></li>
                                <li><form class="navbar-search" action="https://wayofnumbers.com/search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="text" class="search-query" placeholder="Search" name="q" id="tipue_search_input"></form></li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
            <div class="container-fluid">
                <div class="row-fluid">
                    <div class="span1"></div>
                    <div class="span10">
<article itemscope>
<div class="row-fluid">
    <header class="page-header span10 offset2">
    <h1><a href="https://wayofnumbers.com/build-a-fashion-mnist-cnn-pytorch-style-efb297e22582.html"> Letâ€™s Build a Fashion-<span class="caps">MNIST</span> <span class="caps">CNN</span>, PyTorch&nbsp;Style  <small> A Line-by-line guide on how to structure a PyTorch ML project from scratch using Google Colab and TensorBoard </small>  </a></h1>
    </header>
</div>

<div class="row-fluid">
    <div class="span2 table-of-content">
        <nav>
        <h4>Contents</h4>
        <div class="toc">
<ul>
<li><a href="#in-2019-the-war-for-ml-frameworks-has-two-remaining-main-contenders-pytorch-and-tensorflow-my-analysis-suggests-that-researchers-are-abandoning-tensorflow-and-flocking-to-pytorch-in-droves-meanwhile-in-industry-tensorflow-is-currently-the-platform-of-choice-but-that-may-not-be-true-for-long-the-gradient">In 2019, the war for <span class="caps">ML</span> frameworks has two remaining main contenders: PyTorch and TensorFlow. My analysis suggests that researchers are abandoning TensorFlow and flocking to PyTorch in droves. Meanwhile in industry, Tensorflow is currently the platform of choice, but that may not be true for long. â€” The Gradient</a><ul>
<li><a href="#import">Import</a><ul>
<li><a href="#torch">torch</a></li>
<li><a href="#torchnn-and-torchnnfunctional">torch.nn andÂ torch.nn.functional</a></li>
<li><a href="#torchoptim">torch.optim</a></li>
<li><a href="#torchvision">torchvision</a></li>
<li><a href="#summarywriter-tensor-board">SummaryWriter (TensorÂ Board)</a></li>
</ul>
</li>
<li><a href="#dataset">Dataset</a></li>
<li><a href="#network">Network</a></li>
<li><a href="#hyperparameters">Hyperparameters</a><ul>
<li><a href="#runbuilder">RunBuilder</a></li>
<li><a href="#runmanager">RunManager</a></li>
</ul>
</li>
<li><a href="#training">Training</a></li>
<li><a href="#tensor-board">TensorÂ Board</a></li>
<li><a href="#conclusion">Conclusion</a></li>
</ul>
</li>
</ul>
</div>
        </nav>
    </div>
    <div class="span8 article-content">
            
            
<p><img alt="" src="https://cdn-images-1.medium.com/max/4000/1*cz0q6YuI0H8dSd5N9Km5_A.png"/></p>
<p>When it comes to frameworks in technology, one interesting thing is that from the very beginning, there always seems to be a variety of choices. But over time, the competitions will evolve into having only two strong contenders left. Cases in point being â€˜<span class="caps">PC</span> vs Macâ€™, â€˜iOS vs Androidâ€™, â€˜React.js vs Vue.jsâ€™, etc. And now, we have â€˜PyTorch vs TensorFlowâ€™ in machineÂ learning.</p>
<p><a href="https://github.com/tensorflow/tensorflow">TensorFlow</a>, backed by Google, is undoubtedly the front-runner here. Released in 2015 as an open-source machine learning framework, it quickly gained a lot of attention and acceptance, especially in industries where production readiness and deployment is key. <a href="https://github.com/pytorch/pytorch">PyTorch </a>is introduced much later by Facebook in 2017 but quickly gaining a lot of love from practitioners and researchers because of its dynamic computational graph and â€˜<a href="https://legacy.python.org/dev/peps/pep-0020/">pythonic</a>â€™Â style.</p>
<p><img alt="Image from The Gradient" src="https://cdn-images-1.medium.com/max/2360/1*FfvNjEbtpC33GS_sqcW1Kg.png"/><em>Image from <a href="https://thegradient.pub/state-of-ml-frameworks-2019-pytorch-dominates-research-tensorflow-dominates-industry/">TheÂ Gradient</a></em></p>
<p>Recent research by <a href="https://thegradient.pub/state-of-ml-frameworks-2019-pytorch-dominates-research-tensorflow-dominates-industry/">The Gradient </a>shows that PyTorch is doing great with researchers and TensorFlow is dominating the industryÂ world:</p>
<blockquote>
<h1 id="in-2019-the-war-for-ml-frameworks-has-two-remaining-main-contenders-pytorch-and-tensorflow-my-analysis-suggests-that-researchers-are-abandoning-tensorflow-and-flocking-to-pytorch-in-droves-meanwhile-in-industry-tensorflow-is-currently-the-platform-of-choice-but-that-may-not-be-true-for-long-the-gradient">In 2019, the war for <span class="caps">ML</span> frameworks has two remaining main contenders: PyTorch and TensorFlow. My analysis suggests that researchers are abandoning TensorFlow and flocking to PyTorch in droves. Meanwhile in industry, Tensorflow is currently the platform of choice, but that may not be true for long. â€” <a href="https://thegradient.pub/state-of-ml-frameworks-2019-pytorch-dominates-research-tensorflow-dominates-industry/">The Gradient</a><a class="headerlink" href="#in-2019-the-war-for-ml-frameworks-has-two-remaining-main-contenders-pytorch-and-tensorflow-my-analysis-suggests-that-researchers-are-abandoning-tensorflow-and-flocking-to-pytorch-in-droves-meanwhile-in-industry-tensorflow-is-currently-the-platform-of-choice-but-that-may-not-be-true-for-long-the-gradient" title="Permanent link">Â¶</a></h1>
</blockquote>
<p>The recent release of <a href="https://pytorch.org/blog/pytorch-1-dot-3-adds-mobile-privacy-quantization-and-named-tensors/">PyTorch 1.3</a> introduced PyTorch Mobile, quantization and other goodies that are all in the right direction to close the gap. If you are somewhat familiar with neural network basics but want to try PyTorch as a different style, then please read on. Iâ€™ll try to explain how to build a Convolutional Neural Network classifier from scratch for the Fashion-<span class="caps">MNIST</span> dataset using PyTorch. The code here can be used on Google Colab and Tensor Board if you donâ€™t have a powerful local environment. Without further ado, letâ€™s get started. You can find the Google Colab Notebook and GitHub linkÂ below:</p>
<p><a href="https://colab.research.google.com/drive/1YWzAjpAnLI23irBQtLvDTYT1A94uCloM">ðŸ“™ **Google Colab Notebook</a>**</p>
<p>ðŸ‘½ <a href="https://github.com/wayofnumbers/SideProjects/blob/master/PyTorch_Tutorial_Basic_v1.ipynb">**GitHub</a>**</p>
<h2 id="import">Import<a class="headerlink" href="#import" title="Permanent link">Â¶</a></h2>
<p>First, letâ€™s import the necessaryÂ modules.</p>
<div class="highlight"><pre><span></span><span class="c1"># import standard PyTorch modules</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="kn">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="kn">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="kn">as</span> <span class="nn">optim</span>
<span class="kn">from</span> <span class="nn">torch.utils.tensorboard</span> <span class="kn">import</span> <span class="n">SummaryWriter</span> <span class="c1"># TensorBoard support</span>

<span class="c1"># import torchvision module to handle image manipulation</span>
<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="kn">as</span> <span class="nn">transforms</span>

<span class="c1"># calculate train time, writing train data to files etc.</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">clear_output</span>

<span class="n">torch</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">linewidth</span><span class="o">=</span><span class="mi">120</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>     <span class="c1"># On by default, leave it here for clarity</span>
</pre></div>
<p>PyTorch modules are quite straightÂ forward.</p>
<h3 id="torch">torch<a class="headerlink" href="#torch" title="Permanent link">Â¶</a></h3>
<p>torch is the main module that holds all the things you need for <strong>Tensor </strong>computation. You can build a fully functional neural network using Tensor computation alone, but this is not what this article is about. Weâ€™ll make use of the more powerful and convenient torch.nn, torch.optim and torchvision classes to quickly build our <span class="caps">CNN</span>. For those of you interested in knowing how to do this from â€˜<em>scratch scratch</em>â€™, visit this <a href="https://pytorch.org/tutorials/beginner/nn_tutorial.html">fantastic PyTorch official tutoria</a>l by <a href="undefined">Jeremy Howard</a>.</p>
<h3 id="torchnn-and-torchnnfunctional">torch.nn and torch.nn.functional<a class="headerlink" href="#torchnn-and-torchnnfunctional" title="Permanent link">Â¶</a></h3>
<p><img alt="Photo by Alphacolor on Unsplash" src="https://cdn-images-1.medium.com/max/9470/0*zmhp4AMV_HCUa5rH"/><em>Photo by <a href="https://unsplash.com/@duck58cth?utm_source=medium&amp;utm_medium=referral">Alphacolor</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></em></p>
<p>The torch.nnmodule provides many classes and functions to build neural networks. You can think of it as the fundamental building blocks of neural networks: models, all kinds of layers, activation functions, parameter classes, etc. It allows us to build the model like putting some <span class="caps">LEGO</span> setÂ together.</p>
<h3 id="torchoptim">torch.optim<a class="headerlink" href="#torchoptim" title="Permanent link">Â¶</a></h3>
<p>torch.optim offers all the optimizers like <span class="caps">SGD</span>, <span class="caps">ADAM</span>, etc., so you donâ€™t have to write it fromÂ scratch.</p>
<h3 id="torchvision">torchvision<a class="headerlink" href="#torchvision" title="Permanent link">Â¶</a></h3>
<p>torchvision contains a lot of popular datasets, model architectures, and common image transformations for computer vision. We get our Fashion <span class="caps">MNIST</span> dataset from it and also use itsÂ transforms.</p>
<h3 id="summarywriter-tensor-board">SummaryWriter (Tensor Board)<a class="headerlink" href="#summarywriter-tensor-board" title="Permanent link">Â¶</a></h3>
<p>SummaryWriter enables PyTorch to generate the report for Tensor Board. Weâ€™ll use Tensor Board to look at our training data, compare results and gain intuition. Tensor Board used to be TensorFlowâ€™s biggest advantage over PyTorch, but it is now officially supported by PyTorch fromÂ v1.2.</p>
<p>We also imported some other utility modules like time, json, pandas,Â etc.</p>
<h2 id="dataset">Dataset<a class="headerlink" href="#dataset" title="Permanent link">Â¶</a></h2>
<p>torchvision already has the Fashion <span class="caps">MNIST</span> dataset. If youâ€™re not familiar with Fashion <span class="caps">MNIST</span>Â dataset:</p>
<blockquote>
<p>Fashion-<span class="caps">MNIST</span> is a dataset of <a href="https://jobs.zalando.com/tech/">Zalando</a>â€˜s article imagesâ€”consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. We intend Fashion-<span class="caps">MNIST</span> to serve as a direct drop-in replacement for the original <a href="http://yann.lecun.com/exdb/mnist/"><span class="caps">MNIST</span> dataset</a> for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits. â€” <a href="https://github.com/zalandoresearch/fashion-mnist">FromÂ Github</a></p>
</blockquote>
<p><img alt="Fashion MNIST Dataset â€” From GitHub" src="https://cdn-images-1.medium.com/max/3600/1*RCXpLibVCgoRYckEd2kU8Q.png"/><em>Fashion <span class="caps">MNIST</span> Dataset â€” <a href="https://github.com/zalandoresearch/fashion-mnist">FromÂ GitHub</a></em></p>
<div class="highlight"><pre><span></span># Use standard FashionMNIST dataset
train_set = torchvision.datasets.FashionMNIST(
    root = './data/FashionMNIST',
    train = True,
    download = True,
    transform = transforms.Compose([
        transforms.ToTensor()                                 
    ])
)
</pre></div>
<p>This doesnâ€™t need much explanation. We specified the root directory to store the dataset, snatch the training data, allow it to be downloaded if not present at the local machine, and then apply the transforms.ToTensor to turn images into <strong>Tensor </strong>so we can directly use it with our network. The dataset is stored in the dataset class namedÂ train_set.</p>
<h2 id="network">Network<a class="headerlink" href="#network" title="Permanent link">Â¶</a></h2>
<p>Building the actual neural network in PyTorch is fun and easy. I assume you have some basic concept of how a Convolutional Neural Network works. If you donâ€™t, you can refer to this video fromÂ deeplizard:</p>
<p><center><iframe allowfullscreen="" frameborder="0" height="315" src="https://www.youtube.com/embed/videoseries" width="560"></iframe></center></p>
<p>The Fashion <span class="caps">MNIST</span> is only 28x28 px in size, so we actually donâ€™t need a very complicated network. We can just build a simple <span class="caps">CNN</span> likeÂ this:</p>
<p><img alt="" src="https://cdn-images-1.medium.com/max/2098/1*PceBlGfN_G7xyIkAfI7BuA.png"/></p>
<p>We have two convolution layers, each with 5x5 kernels. After each convolution layer, we have a max-pooling layer with a stride of 2. This allows us to extract the necessary features from the images. Then we flatten the tensors and put them into a dense layer, pass through a Multi-Layer Perceptron (<span class="caps">MLP</span>) to carry out the task of classification of our 10Â categories.</p>
<p>Now that we are clear about the structure of the network, letâ€™s see how we can use PyTorch to buildÂ it:</p>
<div class="highlight"><pre><span></span><span class="err">#</span> <span class="nx">Build</span> <span class="nx">the</span> <span class="nx">neural</span> <span class="nx">network</span><span class="p">,</span> <span class="nx">expand</span> <span class="nx">on</span> <span class="nx">top</span> <span class="nx">of</span> <span class="nx">nn</span><span class="p">.</span><span class="nx">Module</span>
<span class="kr">class</span> <span class="nx">Network</span><span class="p">(</span><span class="nx">nn</span><span class="p">.</span><span class="nx">Module</span><span class="p">)</span><span class="o">:</span>
  <span class="nx">def</span> <span class="nx">__init__</span><span class="p">(</span><span class="nx">self</span><span class="p">)</span><span class="o">:</span>
    <span class="kr">super</span><span class="p">().</span><span class="nx">__init__</span><span class="p">()</span>

    <span class="err">#</span> <span class="nx">define</span> <span class="nx">layers</span>
    <span class="nx">self</span><span class="p">.</span><span class="nx">conv1</span> <span class="o">=</span> <span class="nx">nn</span><span class="p">.</span><span class="nx">Conv2d</span><span class="p">(</span><span class="nx">in_channels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="nx">out_channels</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="nx">kernel_size</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="nx">self</span><span class="p">.</span><span class="nx">conv2</span> <span class="o">=</span> <span class="nx">nn</span><span class="p">.</span><span class="nx">Conv2d</span><span class="p">(</span><span class="nx">in_channels</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="nx">out_channels</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="nx">kernel_size</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

    <span class="nx">self</span><span class="p">.</span><span class="nx">fc1</span> <span class="o">=</span> <span class="nx">nn</span><span class="p">.</span><span class="nx">Linear</span><span class="p">(</span><span class="nx">in_features</span><span class="o">=</span><span class="mi">12</span><span class="o">*</span><span class="mi">4</span><span class="o">*</span><span class="mi">4</span><span class="p">,</span> <span class="nx">out_features</span><span class="o">=</span><span class="mi">120</span><span class="p">)</span>
    <span class="nx">self</span><span class="p">.</span><span class="nx">fc2</span> <span class="o">=</span> <span class="nx">nn</span><span class="p">.</span><span class="nx">Linear</span><span class="p">(</span><span class="nx">in_features</span><span class="o">=</span><span class="mi">120</span><span class="p">,</span> <span class="nx">out_features</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>
    <span class="nx">self</span><span class="p">.</span><span class="nx">out</span> <span class="o">=</span> <span class="nx">nn</span><span class="p">.</span><span class="nx">Linear</span><span class="p">(</span><span class="nx">in_features</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span> <span class="nx">out_features</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

  <span class="err">#</span> <span class="nx">define</span> <span class="nx">forward</span> <span class="kd">function</span>
  <span class="nx">def</span> <span class="nx">forward</span><span class="p">(</span><span class="nx">self</span><span class="p">,</span> <span class="nx">t</span><span class="p">)</span><span class="o">:</span>
    <span class="err">#</span> <span class="nx">conv</span> <span class="mi">1</span>
    <span class="nx">t</span> <span class="o">=</span> <span class="nx">self</span><span class="p">.</span><span class="nx">conv1</span><span class="p">(</span><span class="nx">t</span><span class="p">)</span>
    <span class="nx">t</span> <span class="o">=</span> <span class="nx">F</span><span class="p">.</span><span class="nx">relu</span><span class="p">(</span><span class="nx">t</span><span class="p">)</span>
    <span class="nx">t</span> <span class="o">=</span> <span class="nx">F</span><span class="p">.</span><span class="nx">max_pool2d</span><span class="p">(</span><span class="nx">t</span><span class="p">,</span> <span class="nx">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="nx">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="err">#</span> <span class="nx">conv</span> <span class="mi">2</span>
    <span class="nx">t</span> <span class="o">=</span> <span class="nx">self</span><span class="p">.</span><span class="nx">conv2</span><span class="p">(</span><span class="nx">t</span><span class="p">)</span>
    <span class="nx">t</span> <span class="o">=</span> <span class="nx">F</span><span class="p">.</span><span class="nx">relu</span><span class="p">(</span><span class="nx">t</span><span class="p">)</span>
    <span class="nx">t</span> <span class="o">=</span> <span class="nx">F</span><span class="p">.</span><span class="nx">max_pool2d</span><span class="p">(</span><span class="nx">t</span><span class="p">,</span> <span class="nx">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="nx">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="err">#</span> <span class="nx">fc1</span>
    <span class="nx">t</span> <span class="o">=</span> <span class="nx">t</span><span class="p">.</span><span class="nx">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">12</span><span class="o">*</span><span class="mi">4</span><span class="o">*</span><span class="mi">4</span><span class="p">)</span>
    <span class="nx">t</span> <span class="o">=</span> <span class="nx">self</span><span class="p">.</span><span class="nx">fc1</span><span class="p">(</span><span class="nx">t</span><span class="p">)</span>
    <span class="nx">t</span> <span class="o">=</span> <span class="nx">F</span><span class="p">.</span><span class="nx">relu</span><span class="p">(</span><span class="nx">t</span><span class="p">)</span>

    <span class="err">#</span> <span class="nx">fc2</span>
    <span class="nx">t</span> <span class="o">=</span> <span class="nx">self</span><span class="p">.</span><span class="nx">fc2</span><span class="p">(</span><span class="nx">t</span><span class="p">)</span>
    <span class="nx">t</span> <span class="o">=</span> <span class="nx">F</span><span class="p">.</span><span class="nx">relu</span><span class="p">(</span><span class="nx">t</span><span class="p">)</span>

    <span class="err">#</span> <span class="nx">output</span>
    <span class="nx">t</span> <span class="o">=</span> <span class="nx">self</span><span class="p">.</span><span class="nx">out</span><span class="p">(</span><span class="nx">t</span><span class="p">)</span>
    <span class="err">#</span> <span class="nx">don</span><span class="s1">'t need softmax here since we'</span><span class="nx">ll</span> <span class="nx">use</span> <span class="nx">cross</span><span class="o">-</span><span class="nx">entropy</span> <span class="kr">as</span> <span class="nx">activation</span><span class="p">.</span>

    <span class="k">return</span> <span class="nx">t</span>
</pre></div>
<p>First of all, all network classes in PyTorch expand on the base class: nn.Module. It packs all the basics: <strong>weights, biases, forward method</strong> and also some utility attributes and methods like .parameters() and .zero_grad()which we will be usingÂ too.</p>
<p>The structure of our network is defined in the <strong>init</strong> dunderÂ function.</p>
<div class="highlight"><pre><span></span>def __init__(self): 
  super().__init__()

  # define layers 
  self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)
  self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)
  self.fc1 = nn.Linear(in_features=12*4*4, out_features=120)
  self.fc2 = nn.Linear(in_features=120, out_features=60)
  self.out = nn.Linear(in_features=60, out_features=10)
</pre></div>
<p>nn.Conv2d and nn.Linear are two standard PyTorch layers defined within the torch.nn module. These are quite self-explanatory. One thing to note is that we only defined the actual layers here. The activation and max-pooling <strong>operations</strong> are included in the forward function that is explainedÂ below.</p>
<div class="highlight"><pre><span></span># define forward function  
def forward(self, t):  
  # conv 1  
  t = self.conv1(t)  
  t = F.relu(t)  
  t = F.max_pool2d(t, kernel_size=2, stride=2)

  # conv 2  
  t = self.conv2(t)   
  t = F.relu(t)  
  t = F.max_pool2d(t, kernel_size=2, stride=2)

  # fc1   
  t = t.reshape(-1, 12*4*4)  
  t = self.fc1(t)  
  t = F.relu(t)

  # fc2  
  t = self.fc2(t)  
  t = F.relu(t)

  # output  
  t = self.out(t)

  # don't need softmax here since we'll use cross-entropy as activation.   
  return t
</pre></div>
<p>Once the layer is defined, we can then use the layer itself to compute the forward results of each layer, coupled with the activation function(ReLu) and Max Pooling operations, we can easily write the forward function of our network as above. Notice that on fc1(Fully Connect layer 1), we used PyTorchâ€™s tensor operation t.reshape to flatten the tensor so it can be passed to the dense layer afterward. Also, we didnâ€™t add the softmax activation function at the output layer since PyTorchâ€™s <strong>CrossEntropy</strong> function will take care of that forÂ us.</p>
<h2 id="hyperparameters">Hyperparameters<a class="headerlink" href="#hyperparameters" title="Permanent link">Â¶</a></h2>
<p>Normally, we can just handpick one set of hyperparameters and do some experiments with them. In this example, we want to do a bit more by introducing some structuring. Weâ€™ll build a system to generate different hyperparameter combinations and use them to carry out training â€˜runsâ€™. Each â€˜runâ€™ uses one set of hyperparameter combinations. Export the training data/results of each run to Tensor Board so we can directly compare and see which hyperparameters set performs theÂ best.</p>
<p>We store all our hyperparameters in an <a href="https://www.geeksforgeeks.org/ordereddict-in-python/">**OrderedDict</a>**:</p>
<div class="highlight"><pre><span></span># put all hyper params into a OrderedDict, easily expandable
params = OrderedDict(
    lr = [.01, .001],
    batch_size = [100, 1000],
    shuffle = [True, False]
)
epochs = 3
</pre></div>
<p>lr: Learning Rate. We want to try 0.01 and 0.001 for ourÂ models.</p>
<p>batch_size: Batch Size to speed up the training process. Weâ€™ll use 100 andÂ 1000.</p>
<p>shuffle: Shuffle toggle, whether we shuffle the batch beforeÂ training.</p>
<p>Once the parameters are down. We use two helper classes: RunBuilder and RunManager to manage our hyperparameters and trainingÂ process.</p>
<h3 id="runbuilder">RunBuilder<a class="headerlink" href="#runbuilder" title="Permanent link">Â¶</a></h3>
<p>The main purpose of the class RunBuilder is to offer a static method get_runs. It takes the OrderedDict (with all hyperparameters stored in it) as a parameter and generates a <a href="https://www.youtube.com/watch?v=GfxJYp9_nJA">named tuple</a> Run, each element of runrepresent one possible combination of the hyperparameters. This named tuple is later consumed by the training loop. The code is easy toÂ understand.</p>
<div class="highlight"><pre><span></span><span class="c1"># import modules to build RunBuilder and RunManager helper classes</span>
<span class="kn">from</span> <span class="nn">collections</span>  <span class="kn">import</span> <span class="n">OrderedDict</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">namedtuple</span>
<span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">product</span>

<span class="c1"># Read in the hyper-parameters and return a Run namedtuple containing all the </span>
<span class="c1"># combinations of hyper-parameters</span>
<span class="k">class</span> <span class="nc">RunBuilder</span><span class="p">():</span>
  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">get_runs</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>

    <span class="n">Run</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span><span class="s1">'Run'</span><span class="p">,</span> <span class="n">params</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>

    <span class="n">runs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">product</span><span class="p">(</span><span class="o">*</span><span class="n">params</span><span class="o">.</span><span class="n">values</span><span class="p">()):</span>
      <span class="n">runs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Run</span><span class="p">(</span><span class="o">*</span><span class="n">v</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">runs</span>
</pre></div>
<h3 id="runmanager">RunManager<a class="headerlink" href="#runmanager" title="Permanent link">Â¶</a></h3>
<p>There are four main purposes of the RunManagerÂ class.</p>
<ol>
<li>
<p>Calculate and record the duration of each epoch andÂ run.</p>
</li>
<li>
<p>Calculate the training loss and accuracy of each epoch andÂ run.</p>
</li>
<li>
<p>Record the training data (e.g. loss, accuracy, weights, gradients, computational graph, etc.) for each epoch and run, then export them into Tensor Board for furtherÂ analysis.</p>
</li>
<li>
<p>Save all training results in csv and json for future reference or <span class="caps">API</span>Â extraction.</p>
</li>
</ol>
<p>As you can see, it helps us take care of the logistics which is also important for our success in training the model. Letâ€™s look at the code. Itâ€™s a bit long so bear withÂ me:</p>
<div class="highlight"><pre><span></span><span class="err">#</span> <span class="nx">Helper</span> <span class="kr">class</span><span class="p">,</span> <span class="nx">help</span> <span class="nx">track</span> <span class="nx">loss</span><span class="p">,</span> <span class="nx">accuracy</span><span class="p">,</span> <span class="nx">epoch</span> <span class="nx">time</span><span class="p">,</span> <span class="nx">run</span> <span class="nx">time</span><span class="p">,</span> 
<span class="err">#</span> <span class="nx">hyper</span><span class="o">-</span><span class="nx">parameters</span> <span class="nx">etc</span><span class="p">.</span> <span class="nx">Also</span> <span class="nx">record</span> <span class="nx">to</span> <span class="nx">TensorBoard</span> <span class="nx">and</span> <span class="nx">write</span> <span class="nx">into</span> <span class="nx">csv</span><span class="p">,</span> <span class="nx">json</span>
<span class="kr">class</span> <span class="nx">RunManager</span><span class="p">()</span><span class="o">:</span>
  <span class="nx">def</span> <span class="nx">__init__</span><span class="p">(</span><span class="nx">self</span><span class="p">)</span><span class="o">:</span>

    <span class="err">#</span> <span class="nx">tracking</span> <span class="nx">every</span> <span class="nx">epoch</span> <span class="nx">count</span><span class="p">,</span> <span class="nx">loss</span><span class="p">,</span> <span class="nx">accuracy</span><span class="p">,</span> <span class="nx">time</span>
    <span class="nx">self</span><span class="p">.</span><span class="nx">epoch_count</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="nx">self</span><span class="p">.</span><span class="nx">epoch_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="nx">self</span><span class="p">.</span><span class="nx">epoch_num_correct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="nx">self</span><span class="p">.</span><span class="nx">epoch_start_time</span> <span class="o">=</span> <span class="nx">None</span>

    <span class="err">#</span> <span class="nx">tracking</span> <span class="nx">every</span> <span class="nx">run</span> <span class="nx">count</span><span class="p">,</span> <span class="nx">run</span> <span class="nx">data</span><span class="p">,</span> <span class="nx">hyper</span><span class="o">-</span><span class="nx">params</span> <span class="nx">used</span><span class="p">,</span> <span class="nx">time</span>
    <span class="nx">self</span><span class="p">.</span><span class="nx">run_params</span> <span class="o">=</span> <span class="nx">None</span>
    <span class="nx">self</span><span class="p">.</span><span class="nx">run_count</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="nx">self</span><span class="p">.</span><span class="nx">run_data</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="nx">self</span><span class="p">.</span><span class="nx">run_start_time</span> <span class="o">=</span> <span class="nx">None</span>

    <span class="err">#</span> <span class="nx">record</span> <span class="nx">model</span><span class="p">,</span> <span class="nx">loader</span> <span class="nx">and</span> <span class="nx">TensorBoard</span> 
    <span class="nx">self</span><span class="p">.</span><span class="nx">network</span> <span class="o">=</span> <span class="nx">None</span>
    <span class="nx">self</span><span class="p">.</span><span class="nx">loader</span> <span class="o">=</span> <span class="nx">None</span>
    <span class="nx">self</span><span class="p">.</span><span class="nx">tb</span> <span class="o">=</span> <span class="nx">None</span>

  <span class="err">#</span> <span class="nx">record</span> <span class="nx">the</span> <span class="nx">count</span><span class="p">,</span> <span class="nx">hyper</span><span class="o">-</span><span class="nx">param</span><span class="p">,</span> <span class="nx">model</span><span class="p">,</span> <span class="nx">loader</span> <span class="nx">of</span> <span class="nx">each</span> <span class="nx">run</span>
  <span class="err">#</span> <span class="nx">record</span> <span class="nx">sample</span> <span class="nx">images</span> <span class="nx">and</span> <span class="nx">network</span> <span class="nx">graph</span> <span class="nx">to</span> <span class="nx">TensorBoard</span>  
  <span class="nx">def</span> <span class="nx">begin_run</span><span class="p">(</span><span class="nx">self</span><span class="p">,</span> <span class="nx">run</span><span class="p">,</span> <span class="nx">network</span><span class="p">,</span> <span class="nx">loader</span><span class="p">)</span><span class="o">:</span>

    <span class="nx">self</span><span class="p">.</span><span class="nx">run_start_time</span> <span class="o">=</span> <span class="nx">time</span><span class="p">.</span><span class="nx">time</span><span class="p">()</span>

    <span class="nx">self</span><span class="p">.</span><span class="nx">run_params</span> <span class="o">=</span> <span class="nx">run</span>
    <span class="nx">self</span><span class="p">.</span><span class="nx">run_count</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="nx">self</span><span class="p">.</span><span class="nx">network</span> <span class="o">=</span> <span class="nx">network</span>
    <span class="nx">self</span><span class="p">.</span><span class="nx">loader</span> <span class="o">=</span> <span class="nx">loader</span>
    <span class="nx">self</span><span class="p">.</span><span class="nx">tb</span> <span class="o">=</span> <span class="nx">SummaryWriter</span><span class="p">(</span><span class="nx">comment</span><span class="o">=</span><span class="nx">f</span><span class="s1">'-{run}'</span><span class="p">)</span>

    <span class="nx">images</span><span class="p">,</span> <span class="nx">labels</span> <span class="o">=</span> <span class="nx">next</span><span class="p">(</span><span class="nx">iter</span><span class="p">(</span><span class="nx">self</span><span class="p">.</span><span class="nx">loader</span><span class="p">))</span>
    <span class="nx">grid</span> <span class="o">=</span> <span class="nx">torchvision</span><span class="p">.</span><span class="nx">utils</span><span class="p">.</span><span class="nx">make_grid</span><span class="p">(</span><span class="nx">images</span><span class="p">)</span>

    <span class="nx">self</span><span class="p">.</span><span class="nx">tb</span><span class="p">.</span><span class="nx">add_image</span><span class="p">(</span><span class="s1">'images'</span><span class="p">,</span> <span class="nx">grid</span><span class="p">)</span>
    <span class="nx">self</span><span class="p">.</span><span class="nx">tb</span><span class="p">.</span><span class="nx">add_graph</span><span class="p">(</span><span class="nx">self</span><span class="p">.</span><span class="nx">network</span><span class="p">,</span> <span class="nx">images</span><span class="p">)</span>

  <span class="err">#</span> <span class="nx">when</span> <span class="nx">run</span> <span class="nx">ends</span><span class="p">,</span> <span class="nx">close</span> <span class="nx">TensorBoard</span><span class="p">,</span> <span class="nx">zero</span> <span class="nx">epoch</span> <span class="nx">count</span>
  <span class="nx">def</span> <span class="nx">end_run</span><span class="p">(</span><span class="nx">self</span><span class="p">)</span><span class="o">:</span>
    <span class="nx">self</span><span class="p">.</span><span class="nx">tb</span><span class="p">.</span><span class="nx">close</span><span class="p">()</span>
    <span class="nx">self</span><span class="p">.</span><span class="nx">epoch_count</span> <span class="o">=</span> <span class="mi">0</span>

  <span class="err">#</span> <span class="nx">zero</span> <span class="nx">epoch</span> <span class="nx">count</span><span class="p">,</span> <span class="nx">loss</span><span class="p">,</span> <span class="nx">accuracy</span><span class="p">,</span> 
  <span class="nx">def</span> <span class="nx">begin_epoch</span><span class="p">(</span><span class="nx">self</span><span class="p">)</span><span class="o">:</span>
    <span class="nx">self</span><span class="p">.</span><span class="nx">epoch_start_time</span> <span class="o">=</span> <span class="nx">time</span><span class="p">.</span><span class="nx">time</span><span class="p">()</span>

    <span class="nx">self</span><span class="p">.</span><span class="nx">epoch_count</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="nx">self</span><span class="p">.</span><span class="nx">epoch_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="nx">self</span><span class="p">.</span><span class="nx">epoch_num_correct</span> <span class="o">=</span> <span class="mi">0</span>

  <span class="err">#</span> 
  <span class="nx">def</span> <span class="nx">end_epoch</span><span class="p">(</span><span class="nx">self</span><span class="p">)</span><span class="o">:</span>
    <span class="err">#</span> <span class="nx">calculate</span> <span class="nx">epoch</span> <span class="nx">duration</span> <span class="nx">and</span> <span class="nx">run</span> <span class="nx">duration</span><span class="p">(</span><span class="nx">accumulate</span><span class="p">)</span>
    <span class="nx">epoch_duration</span> <span class="o">=</span> <span class="nx">time</span><span class="p">.</span><span class="nx">time</span><span class="p">()</span> <span class="o">-</span> <span class="nx">self</span><span class="p">.</span><span class="nx">epoch_start_time</span>
    <span class="nx">run_duration</span> <span class="o">=</span> <span class="nx">time</span><span class="p">.</span><span class="nx">time</span><span class="p">()</span> <span class="o">-</span> <span class="nx">self</span><span class="p">.</span><span class="nx">run_start_time</span>

    <span class="err">#</span> <span class="nx">record</span> <span class="nx">epoch</span> <span class="nx">loss</span> <span class="nx">and</span> <span class="nx">accuracy</span>
    <span class="nx">loss</span> <span class="o">=</span> <span class="nx">self</span><span class="p">.</span><span class="nx">epoch_loss</span> <span class="o">/</span> <span class="nx">len</span><span class="p">(</span><span class="nx">self</span><span class="p">.</span><span class="nx">loader</span><span class="p">.</span><span class="nx">dataset</span><span class="p">)</span>
    <span class="nx">accuracy</span> <span class="o">=</span> <span class="nx">self</span><span class="p">.</span><span class="nx">epoch_num_correct</span> <span class="o">/</span> <span class="nx">len</span><span class="p">(</span><span class="nx">self</span><span class="p">.</span><span class="nx">loader</span><span class="p">.</span><span class="nx">dataset</span><span class="p">)</span>

    <span class="err">#</span> <span class="nx">Record</span> <span class="nx">epoch</span> <span class="nx">loss</span> <span class="nx">and</span> <span class="nx">accuracy</span> <span class="nx">to</span> <span class="nx">TensorBoard</span> 
    <span class="nx">self</span><span class="p">.</span><span class="nx">tb</span><span class="p">.</span><span class="nx">add_scalar</span><span class="p">(</span><span class="s1">'Loss'</span><span class="p">,</span> <span class="nx">loss</span><span class="p">,</span> <span class="nx">self</span><span class="p">.</span><span class="nx">epoch_count</span><span class="p">)</span>
    <span class="nx">self</span><span class="p">.</span><span class="nx">tb</span><span class="p">.</span><span class="nx">add_scalar</span><span class="p">(</span><span class="s1">'Accuracy'</span><span class="p">,</span> <span class="nx">accuracy</span><span class="p">,</span> <span class="nx">self</span><span class="p">.</span><span class="nx">epoch_count</span><span class="p">)</span>

    <span class="err">#</span> <span class="nx">Record</span> <span class="nx">params</span> <span class="nx">to</span> <span class="nx">TensorBoard</span>
    <span class="k">for</span> <span class="nx">name</span><span class="p">,</span> <span class="nx">param</span> <span class="k">in</span> <span class="nx">self</span><span class="p">.</span><span class="nx">network</span><span class="p">.</span><span class="nx">named_parameters</span><span class="p">()</span><span class="o">:</span>
      <span class="nx">self</span><span class="p">.</span><span class="nx">tb</span><span class="p">.</span><span class="nx">add_histogram</span><span class="p">(</span><span class="nx">name</span><span class="p">,</span> <span class="nx">param</span><span class="p">,</span> <span class="nx">self</span><span class="p">.</span><span class="nx">epoch_count</span><span class="p">)</span>
      <span class="nx">self</span><span class="p">.</span><span class="nx">tb</span><span class="p">.</span><span class="nx">add_histogram</span><span class="p">(</span><span class="nx">f</span><span class="s1">'{name}.grad'</span><span class="p">,</span> <span class="nx">param</span><span class="p">.</span><span class="nx">grad</span><span class="p">,</span> <span class="nx">self</span><span class="p">.</span><span class="nx">epoch_count</span><span class="p">)</span>

    <span class="err">#</span> <span class="nx">Write</span> <span class="nx">into</span> <span class="s1">'results'</span> <span class="p">(</span><span class="nx">OrderedDict</span><span class="p">)</span> <span class="k">for</span> <span class="nx">all</span> <span class="nx">run</span> <span class="nx">related</span> <span class="nx">data</span>
    <span class="nx">results</span> <span class="o">=</span> <span class="nx">OrderedDict</span><span class="p">()</span>
    <span class="nx">results</span><span class="p">[</span><span class="s2">"run"</span><span class="p">]</span> <span class="o">=</span> <span class="nx">self</span><span class="p">.</span><span class="nx">run_count</span>
    <span class="nx">results</span><span class="p">[</span><span class="s2">"epoch"</span><span class="p">]</span> <span class="o">=</span> <span class="nx">self</span><span class="p">.</span><span class="nx">epoch_count</span>
    <span class="nx">results</span><span class="p">[</span><span class="s2">"loss"</span><span class="p">]</span> <span class="o">=</span> <span class="nx">loss</span>
    <span class="nx">results</span><span class="p">[</span><span class="s2">"accuracy"</span><span class="p">]</span> <span class="o">=</span> <span class="nx">accuracy</span>
    <span class="nx">results</span><span class="p">[</span><span class="s2">"epoch duration"</span><span class="p">]</span> <span class="o">=</span> <span class="nx">epoch_duration</span>
    <span class="nx">results</span><span class="p">[</span><span class="s2">"run duration"</span><span class="p">]</span> <span class="o">=</span> <span class="nx">run_duration</span>

    <span class="err">#</span> <span class="nx">Record</span> <span class="nx">hyper</span><span class="o">-</span><span class="nx">params</span> <span class="nx">into</span> <span class="s1">'results'</span>
    <span class="k">for</span> <span class="nx">k</span><span class="p">,</span><span class="nx">v</span> <span class="k">in</span> <span class="nx">self</span><span class="p">.</span><span class="nx">run_params</span><span class="p">.</span><span class="nx">_asdict</span><span class="p">().</span><span class="nx">items</span><span class="p">()</span><span class="o">:</span> <span class="nx">results</span><span class="p">[</span><span class="nx">k</span><span class="p">]</span> <span class="o">=</span> <span class="nx">v</span>
    <span class="nx">self</span><span class="p">.</span><span class="nx">run_data</span><span class="p">.</span><span class="nx">append</span><span class="p">(</span><span class="nx">results</span><span class="p">)</span>
    <span class="nx">df</span> <span class="o">=</span> <span class="nx">pd</span><span class="p">.</span><span class="nx">DataFrame</span><span class="p">.</span><span class="nx">from_dict</span><span class="p">(</span><span class="nx">self</span><span class="p">.</span><span class="nx">run_data</span><span class="p">,</span> <span class="nx">orient</span> <span class="o">=</span> <span class="s1">'columns'</span><span class="p">)</span>

    <span class="err">#</span> <span class="nx">display</span> <span class="nx">epoch</span> <span class="nx">information</span> <span class="nx">and</span> <span class="nx">show</span> <span class="nx">progress</span>
    <span class="nx">clear_output</span><span class="p">(</span><span class="nx">wait</span><span class="o">=</span><span class="nx">True</span><span class="p">)</span>
    <span class="nx">display</span><span class="p">(</span><span class="nx">df</span><span class="p">)</span>

  <span class="err">#</span> <span class="nx">accumulate</span> <span class="nx">loss</span> <span class="nx">of</span> <span class="nx">batch</span> <span class="nx">into</span> <span class="nx">entire</span> <span class="nx">epoch</span> <span class="nx">loss</span>
  <span class="nx">def</span> <span class="nx">track_loss</span><span class="p">(</span><span class="nx">self</span><span class="p">,</span> <span class="nx">loss</span><span class="p">)</span><span class="o">:</span>
    <span class="err">#</span> <span class="nx">multiply</span> <span class="nx">batch</span> <span class="nx">size</span> <span class="nx">so</span> <span class="nx">variety</span> <span class="nx">of</span> <span class="nx">batch</span> <span class="nx">sizes</span> <span class="nx">can</span> <span class="nx">be</span> <span class="nx">compared</span>
    <span class="nx">self</span><span class="p">.</span><span class="nx">epoch_loss</span> <span class="o">+=</span> <span class="nx">loss</span><span class="p">.</span><span class="nx">item</span><span class="p">()</span> <span class="o">*</span> <span class="nx">self</span><span class="p">.</span><span class="nx">loader</span><span class="p">.</span><span class="nx">batch_size</span>

  <span class="err">#</span> <span class="nx">accumulate</span> <span class="kt">number</span> <span class="nx">of</span> <span class="nx">corrects</span> <span class="nx">of</span> <span class="nx">batch</span> <span class="nx">into</span> <span class="nx">entire</span> <span class="nx">epoch</span> <span class="nx">num_correct</span>
  <span class="nx">def</span> <span class="nx">track_num_correct</span><span class="p">(</span><span class="nx">self</span><span class="p">,</span> <span class="nx">preds</span><span class="p">,</span> <span class="nx">labels</span><span class="p">)</span><span class="o">:</span>
    <span class="nx">self</span><span class="p">.</span><span class="nx">epoch_num_correct</span> <span class="o">+=</span> <span class="nx">self</span><span class="p">.</span><span class="nx">_get_num_correct</span><span class="p">(</span><span class="nx">preds</span><span class="p">,</span> <span class="nx">labels</span><span class="p">)</span>

  <span class="kd">@torch</span><span class="p">.</span><span class="nx">no_grad</span><span class="p">()</span>
  <span class="nx">def</span> <span class="nx">_get_num_correct</span><span class="p">(</span><span class="nx">self</span><span class="p">,</span> <span class="nx">preds</span><span class="p">,</span> <span class="nx">labels</span><span class="p">)</span><span class="o">:</span>
    <span class="k">return</span> <span class="nx">preds</span><span class="p">.</span><span class="nx">argmax</span><span class="p">(</span><span class="nx">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="nx">eq</span><span class="p">(</span><span class="nx">labels</span><span class="p">).</span><span class="nx">sum</span><span class="p">().</span><span class="nx">item</span><span class="p">()</span>

  <span class="err">#</span> <span class="nx">save</span> <span class="nx">end</span> <span class="nx">results</span> <span class="nx">of</span> <span class="nx">all</span> <span class="nx">runs</span> <span class="nx">into</span> <span class="nx">csv</span><span class="p">,</span> <span class="nx">json</span> <span class="k">for</span> <span class="nx">further</span> <span class="nx">analysis</span>
  <span class="nx">def</span> <span class="nx">save</span><span class="p">(</span><span class="nx">self</span><span class="p">,</span> <span class="nx">fileName</span><span class="p">)</span><span class="o">:</span>

    <span class="nx">pd</span><span class="p">.</span><span class="nx">DataFrame</span><span class="p">.</span><span class="nx">from_dict</span><span class="p">(</span>
        <span class="nx">self</span><span class="p">.</span><span class="nx">run_data</span><span class="p">,</span> 
        <span class="nx">orient</span> <span class="o">=</span> <span class="s1">'columns'</span><span class="p">,</span>
    <span class="p">).</span><span class="nx">to_csv</span><span class="p">(</span><span class="nx">f</span><span class="s1">'{fileName}.csv'</span><span class="p">)</span>

    <span class="kd">with</span> <span class="nx">open</span><span class="p">(</span><span class="nx">f</span><span class="s1">'{fileName}.json'</span><span class="p">,</span> <span class="s1">'w'</span><span class="p">,</span> <span class="nx">encoding</span><span class="o">=</span><span class="s1">'utf-8'</span><span class="p">)</span> <span class="kr">as</span> <span class="nx">f</span>:
      <span class="kt">json.dump</span><span class="p">(</span><span class="nx">self</span><span class="p">.</span><span class="nx">run_data</span><span class="p">,</span> <span class="nx">f</span><span class="p">,</span> <span class="nx">ensure_ascii</span><span class="o">=</span><span class="nx">False</span><span class="p">,</span> <span class="nx">indent</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
<p><strong><strong>init</strong></strong>: Initialize necessary attributes like count, loss, number of correct predictions, start time,Â etc.</p>
<p><strong>begin_run</strong>: Record run start time so when a run is finished, the duration of the run can be calculated. Create a SummaryWriter object to store everything we want to export into Tensor Board during the run. Write the network graph and sample images into the SummaryWriterÂ object.</p>
<p><strong>end_run</strong>: When run is finished, close the SummaryWriter object and reset the epoch count to 0 (getting ready for nextÂ run).</p>
<p><strong>begin_epoch</strong>: Record epoch start time so epoch duration can be calculated when epoch ends. Reset epoch_loss andÂ epoch_num_correct.</p>
<p><strong>end_epoch</strong>: This function is where most things happen. When an epoch ends, weâ€™ll calculate the epoch duration and the run duration(up to this epoch, not the final run duration unless for the last epoch of the run). Weâ€™ll calculate the total loss and accuracy for this epoch, then export the loss, accuracy, weights/biases, gradients we recorded into Tensor Board. For ease of tracking within the Jupyter Notebook, we also created an <strong>OrderedDict</strong> object results and put all our run data(loss, accuracy, run count, epoch count, run duration, epoch duration, all hyperparameters) into it. Then weâ€™ll use <strong>Pandas </strong>to read it in and display it in a neat tableÂ format.</p>
<p><strong>track_loss, track_num_correct, _get_num_correct</strong>: These are utility functions to accumulate the loss, number of correct predictions of each batch so the epoch loss and accuracy can be calculatedÂ later.</p>
<p><strong>save</strong>: Save all run data (a list of results <strong>OrderedDict </strong>objects for all runs) into csv and json format for further analysis or <span class="caps">API</span>Â access.</p>
<p>There is a lot to take in for this RunManager class. Congrats on coming to this far! The hardest part is already behind you. From now on everything will start to come together and makeÂ sense.</p>
<h2 id="training">Training<a class="headerlink" href="#training" title="Permanent link">Â¶</a></h2>
<p>Finally, we are ready to do some training! With the help of our RunBuilder and RunManager classes, the training process is aÂ breeze:</p>
<div class="highlight"><pre><span></span>m = RunManager()

# get all runs from params using RunBuilder class
for run in RunBuilder.get_runs(params):

    # if params changes, following line of code should reflect the changes too
    network = Network()
    loader = torch.utils.data.DataLoader(train_set, batch_size = run.batch_size)
    optimizer = optim.Adam(network.parameters(), lr=run.lr)

    m.begin_run(run, network, loader)
    for epoch in range(epochs):

      m.begin_epoch()
      for batch in loader:

        images = batch[0]
        labels = batch[1]
        preds = network(images)
        loss = F.cross_entropy(preds, labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        m.track_loss(loss)
        m.track_num_correct(preds, labels)

      m.end_epoch()
    m.end_run()

# when all runs are done, save results to files
m.save('results')
</pre></div>
<p>First, we use RunBuilder to create an iterator of hyperparameters, then loop through each hyperparameter combination to carry out ourÂ training:</p>
<div class="highlight"><pre><span></span>for run in RunBuilder.get_runs(params):
</pre></div>
<p>Then, we create our network object from the Network class defined above. network = Network() . This network objects hold all our weights/biases we need toÂ train.</p>
<p>We also need to create a DataLoader object. It is a PyTorch class that holds our training/validation/test dataset, and it will iterate through the dataset and gives us training data in batches equal to the batch_sizeÂ specied.</p>
<div class="highlight"><pre><span></span>loader = torch.utils.data.DataLoader(train_set, batch_size = run.batch_size)
</pre></div>
<p>After that, weâ€™ll create an optimizer using torch.optim class. The optim class gets network parameters and learning rate as input and will help us step through the training process and updates the gradients, etc. Weâ€™ll use Adam as our optimization algorithmÂ here.</p>
<div class="highlight"><pre><span></span>optimizer = optim.Adam(network.parameters(), lr=run.lr)
</pre></div>
<p><span class="caps">OK</span>. Now we have our network created, data loader prepared and optimizer chosen. Letâ€™s get the trainingÂ rolling!</p>
<p>We will loop through all the epochs we want (3 here) to train, so we wrap everything in an â€˜epochâ€™ loop. We also use the begin_run method of our RunManager class to start tracking run trainingÂ data.</p>
<div class="highlight"><pre><span></span>m.begin_run(run, network, loader)    
for epoch in range(epochs):
</pre></div>
<p>For each epoch, weâ€™ll loop through each batch of images to carry out theÂ training.</p>
<div class="highlight"><pre><span></span>m.begin_epoch()    
for batch in loader:              
  images = batch[0]      
  labels = batch[1]      
  preds = network(images)      
  loss = F.cross_entropy(preds, labels)

  optimizer.zero_grad()  
  loss.backward()      
  optimizer.step()

  m.track_loss(loss)      
  m.track_num_correct(preds, labels)
</pre></div>
<p>The above code is where real training happens. We read in the images and labels from the batch, use network class to do the forward propagation (remember the forward method above?) and get the predictions. With predictions, we can calculate the loss of this batch using cross_entropy function. Once the loss is calculated, we reset the gradients (otherwise PyTorch will accumulate the gradients which is not what we want) with .zero_grad(), do one back propagation use loss.backward()method to calculate all the gradients of the weights/biases. Then, we use the optimizer defined above to update the weights/biases. Now that the network is updated for the current batch, weâ€™ll calculate the loss and number of correct predictions and accumulate/track them using track_loss and track_num_correct methods of our RunManagerÂ class.</p>
<p>Once all is finished, weâ€™ll save the results in filesÂ usingm.save(â€˜resultsâ€™).</p>
<p>The output of the runs in the notebook looks likeÂ this:</p>
<p><img alt="" src="https://cdn-images-1.medium.com/max/2000/1*qgEb42V3ubSA9P8BKryv2w.png"/></p>
<h2 id="tensor-board">Tensor Board<a class="headerlink" href="#tensor-board" title="Permanent link">Â¶</a></h2>
<p><img alt="Image from Tensorboard.org" src="https://cdn-images-1.medium.com/max/2000/0*dOWdO9wmi8tNgjKF.gif"/><em>Image fromÂ Tensorboard.org</em></p>
<p>Tensor Board is a TensorFlow visualization tool now also supported by PyTorch. Weâ€™ve already taken the efforts to export everything into the â€˜./runsâ€™ folder where Tensor Board will be looking into for records to consume. What we need to do now is just to launch the Tensor Board and check. Since Iâ€™m running this model on Google Colab, weâ€™ll use a service called ngrok to proxy and access our Tensor Board running on Colab virtual machine. Install ngrokÂ first:</p>
<div class="highlight"><pre><span></span><span class="sx">!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip</span>

<span class="sx">!unzip ngrok-stable-linux-amd64.zip</span>
</pre></div>
<p>Then, specify the folder we want to run Tensor Board from and launch the Tensor Board web interface (./runs is theÂ default):</p>
<div class="highlight"><pre><span></span>LOG_DIR = './runs'

get_ipython().system_raw(

'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &amp;'

.format(LOG_DIR)

)
</pre></div>
<p>Launch ngrokÂ proxy:</p>
<div class="highlight"><pre><span></span>get_ipython().system_raw('./ngrok http 6006 &amp;')
</pre></div>
<p>Generate an <span class="caps">URL</span> so we can access our Tensor Board from within the JupyterÂ Notebook:</p>
<div class="highlight"><pre><span></span><span class="err">!</span> <span class="n">curl</span> <span class="o">-</span><span class="n">s</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">localhost</span><span class="p">:</span><span class="mi">4040</span><span class="o">/</span><span class="n">api</span><span class="o">/</span><span class="n">tunnels</span> <span class="o">|</span> <span class="n">python3</span> <span class="o">-</span><span class="n">c</span> \

<span class="s2">"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])"</span>
</pre></div>
<p>As we can see below, TensorBoard is a very convenient visualization tool for us to get insights into our training and can help greatly with the hyperparameter tuning process. We can easily spot which hyperparameter comp performs the best and then using it to do our realÂ training.</p>
<p><img alt="" src="https://cdn-images-1.medium.com/max/2000/1*L5xNeZ1A4cENWc0tmvbwWg.png"/></p>
<p><img alt="" src="https://cdn-images-1.medium.com/max/2000/1*jjAFS756-DntXAgu_ryMdA.png"/></p>
<p><img alt="" src="https://cdn-images-1.medium.com/max/2224/1*m_ap-x42L9h53Phfa47ZtA.png"/></p>
<h2 id="conclusion">Conclusion<a class="headerlink" href="#conclusion" title="Permanent link">Â¶</a></h2>
<p>As you can see, PyTorch as a machine learning framework is flexible, powerful and expressive. You just write Python code. Since the main focus of this article is to showcase how to use PyTorch to build a Convolutional Neural Network and training it in a structured way, I didnâ€™t finish the whole training epochs and the accuracy is not optimum. You can try it yourself and see how well the modelÂ performs.</p>
<p>This article is heavily inspired by <a href="https://towardsdatascience.com/hidden-gem-a-great-pytorch-youtube-tutorial-series-by-deeplizard-8de677411bc5">deeplizardâ€™s PyTorch video series on YouTube</a>. Even most of the code snippets are directly copied from it. Iâ€™d like to thank them for the great content and if you feel the need to delve down deeper, feel free to go check it out and subscribe to theirÂ channel.</p>


            <div>
            <span class="author_blurb"><a href=""><span class="author_name">Michael Li</span></a> -
                </span><br />
</div>

            
            <section>
<p id="comment-message">So what do you think? Did I miss anything? Is any part unclear? Leave your comments below. </p>
<div class="accordion" id="accordion2">
    <div class="accordion-group">
        <div class="accordion-heading">
            <a class="accordion-toggle disqus-comment-count" data-toggle="collapse" data-parent="#accordion2"
                href="https://wayofnumbers.com/build-a-fashion-mnist-cnn-pytorch-style-efb297e22582.html#disqus_thread",
                id="disqus-accordion-toggle">
                Comments
            </a>
        </div>
        <div id="disqus_thread" class="accordion-body collapse">
            <div class="accordion-inner">
                <div class="comments">
                    <div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'way-of-numbers';
        var disqus_identifier = 'https://wayofnumbers.com/build-a-fashion-mnist-cnn-pytorch-style-efb297e22582.html';
    var disqus_url = 'https://wayofnumbers.com/build-a-fashion-mnist-cnn-pytorch-style-efb297e22582.html';

    (function() {
         var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
         dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
         (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
     })();
</script>
<noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

                </div>
            </div>
        </div>
    </div>
</div>
</section>

            <hr/>
            <aside>
            <nav>
            <ul class="articles-timeline">
                <li class="previous-article">Â« <a href="https://wayofnumbers.com/how-to-build-your-own-pytorch-neural-network-layer-from-scratch-842144d623f6.html" title="Previous: How to Build Your Own PyTorch Neural Network Layer fromÂ Scratch - And learn a thing or two about weight initialization">How to Build Your Own PyTorch Neural Network Layer fromÂ Scratch <small>And learn a thing or two about weight initialization</small></a></li>
            </ul>
            </nav>
            </aside>
        </div>
        <section>
        <div class="span2" style="float:right;font-size:0.9em;">
            <h4>Published</h4>
            <time itemprop="dateCreated" datetime="2020-01-30T11:13:57-06:00">Jan 30, 2020</time>
            <h4>Category</h4>
            <a class="category-link" href="https://wayofnumbers.com/categories.html#machine-learning-ref">Machine Learning</a>
            <h4>Tags</h4>
            <ul class="list-of-tags tags-in-article">
                <li><a href="https://wayofnumbers.com/tags#artificial-intelligence-ref">Artificial Intelligence
                    <span>7</span>
</a></li>
                <li><a href="https://wayofnumbers.com/tags#machine-learning-ref">Machine Learning
                    <span>20</span>
</a></li>
            </ul>
<h4>Contact</h4>
    <a href="https://twitter.com/lymenlee" title="My Twitter Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-twitter sidebar-social-links"></i></a>
    <a href="https://medium.com/@lymenlee" title="My Medium Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-medium sidebar-social-links"></i></a>
    <a href="https://github.com/wayofnumbers" title="My GitHub Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-github sidebar-social-links"></i></a>
    <a href="www.linkedin.com/in/michael-li-dfw" title="My LinkedIn Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-linkedin sidebar-social-links"></i></a>
<!-- Begin MailChimp Signup Form -->
<div id="mc-embed-signup">
<form action="https://github.us17.list-manage.com/subscribe/post?u=c212184cc0965bdf1658f69f0&amp;id=5677a7b75e" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
<h4>Get Monthly Updates</h4>
<input type="email" value="" name="EMAIL" class="email" id="mce-EMAIL" placeholder="Enter your email..." required>
<div class="clear"><input type="submit" value="Send me Free Updates" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
</form>
</div>
<!--End mc_embed_signup-->
        </div>
        </section>
</div>
</article>
                    </div>
                    <div class="span1"></div>
                </div>
            </div>
        </div>
<footer>

    <div>
        <span class="site-name">Way of Numbers</span> - Data science for the rest of us.
    </div>


    <div id="fpowered">
        Powered by: <a href="http://getpelican.com/" title="Pelican Home Page" target="_blank" rel="nofollow">Pelican</a>
        Theme: <a href="https://elegant.oncrashreboot.com/" title="Theme Elegant Home Page" target="_blank" rel="nofollow">Elegant</a>
    </div>
</footer>            <script src="//code.jquery.com/jquery.min.js"></script>
        <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/js/bootstrap.min.js"></script>
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>

            <script type="text/javascript">
var disqus_shortname = 'way-of-numbers';
(function () {
    var s = document.createElement('script'); s.async = true;
    s.type = 'text/javascript';
    s.src = '//' + disqus_shortname + '.disqus.com/count.js';
    (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
}());
</script>
<script  language="javascript" type="text/javascript">
function uncollapse() {
    if (window.location.hash.match(/^#comment-\d+$/)) {
        $('#disqus_thread').collapse('show');
    }
}
</script>
<script type="text/javascript" language="JavaScript">
uncollapse();
window.onhashchange=function(){
    if (window.location.hash.match(/^#comment-\d+$/))
        window.location.reload(true);
}
</script>
<script>
$('#disqus_thread').on('shown', function () {
    var link = document.getElementById('disqus-accordion-toggle');
    var old_innerHTML = link.innerHTML;
    $(link).fadeOut(500, function() {
        $(this).text('Click here to hide comments').fadeIn(500);
    });
    $('#disqus_thread').on('hidden', function () {
        $(link).fadeOut(500, function() {
            $(this).text(old_innerHTML).fadeIn(500);
        });
    })
})
</script>


    </body>
    <!-- Theme: Elegant built for Pelican
        License : MIT -->
</html>