<!DOCTYPE html>
<html lang="en">
<head>
          <title>Way of Numbers - How I Trained Computer to Learn Calligraphy Styles: Part&nbsp;1</title>
        <meta charset="utf-8" />
        <link href="https://wayofnumbers.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Way of Numbers Full Atom Feed" />
        <link href="https://wayofnumbers.github.io/feeds/machine-learning.atom.xml" type="application/atom+xml" rel="alternate" title="Way of Numbers Categories Atom Feed" />




    <meta name="tags" content="Machine Learning" />
    <meta name="tags" content="AI" />
    <meta name="tags" content="Deep Learning" />
    <meta name="tags" content="fast.ai" />
    <meta name="tags" content="calligraphy" />

</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="https://wayofnumbers.github.io/">Way of Numbers <strong>Data science for the rest of us.</strong></a></h1>
        </header><!-- /#banner -->
        <nav id="menu"><ul>
            <li><a href="/">Homepage</a></li>
            <li><a href="/categories.html">Categories</a></li>
            <li><a href="./pages/about.html">About</a></li>
            <li class="active"><a href="https://wayofnumbers.github.io/category/machine-learning.html">Machine Learning</a></li>
            <li><a href="https://wayofnumbers.github.io/category/tools.html">Tools</a></li>
        </ul></nav><!-- /#menu -->
<section id="content" class="body">
  <header>
    <h2 class="entry-title">
      <a href="https://wayofnumbers.github.io/chinese-calligraphy-classifier-1.html" rel="bookmark"
         title="Permalink to How I Trained Computer to Learn Calligraphy Styles: PartÂ 1">How I Trained Computer to Learn Calligraphy Styles: Part&nbsp;1</a></h2>
 
  </header>
  <footer class="post-info">
    <time class="published" datetime="2019-09-15T20:00:00-05:00">
      Sun 15 September 2019
    </time>
    <address class="vcard author">
      By           <a class="url fn" href="https://wayofnumbers.github.io/author/michael-li.html">Michael Li</a>
    </address>
    <div class="category">
        Category: <a href="https://wayofnumbers.github.io/category/machine-learning.html">Machine Learning</a>
    </div>
    <div class="tags">
        Tags:
            <a href="https://wayofnumbers.github.io/tag/machine-learning.html">Machine Learning</a>
            <a href="https://wayofnumbers.github.io/tag/ai.html">AI</a>
            <a href="https://wayofnumbers.github.io/tag/deep-learning.html">Deep Learning</a>
            <a href="https://wayofnumbers.github.io/tag/fastai.html">fast.ai</a>
            <a href="https://wayofnumbers.github.io/tag/calligraphy.html">calligraphy</a>
    </div>
  </footer><!-- /.post-info -->
  <div class="entry-content">
    
<p>Build a Deep Learning Model with fast.aiÂ Library</p>
<p><img alt="Photo by Raychan on Unsplash" src="https://cdn-images-1.medium.com/max/10944/0*1vRfrkhsQiTkkBgJ"/><em>Photo by <a href="https://unsplash.com/@wx1993?utm_source=medium&amp;utm_medium=referral">Raychan</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></em></p>
<p>I wanted to start a series of posts for the projects I finished/polished for my <a href="https://course.fast.ai/">Practical Deep Learning for Coders</a> fast.ai course. Since Iâ€™m pretty green on <span class="caps">ML</span>/<span class="caps">DL</span> field, I hope the challenges I faced and overcome could be of value for other people experiencing the sameÂ journey.</p>
<p>Model <a href="https://medium.com/@lymenlee/deep-learning-models-by-fast-ai-library-c1cccc13e2b3">1</a> ãƒ»<a href="https://medium.com/datadriveninvestor/chinese-calligraphy-classifier-fine-tuning-cbfbf0e304d8">1a</a></p>
<h3 id="why-build-a-chinese-calligraphy-classifier"><strong>Why Build a Chinese Calligraphy Classifier</strong><a class="headerlink" href="#why-build-a-chinese-calligraphy-classifier" title="Permanent link">Â¶</a></h3>
<p>Like any calligraphy, Chinese calligraphy is a form of art. Some great pieces written by some ancient masters have both great art value and economic values (selling at multi-million dollars onÂ auctions).</p>
<p><img alt="*Jieshi Tie* by Song Dynasty politician and scholar Zeng Gong, $30,000,000" src="https://cdn-images-1.medium.com/max/2000/1*2lrTyRMYIcm6HfnojdgUvg.jpeg"/>*<em>Jieshi Tie</em> by Song Dynasty politician and scholar Zeng Gong,Â \$30,000,000*</p>
<p>There are multiple main styles/schools of calligraphy, mainly belongs to different dynasties. Each has its own way of shaping the character and arranging them. The differences are subtle and abstract. It makes sense to see if a trained deep learning model can do a good job of telling which style it is.
<a href="https://www.datadriveninvestor.com/2019/03/03/editors-pick-5-machine-learning-books/"><strong><span class="caps">DDI</span> Editorâ€™s Pick: 5 Machine Learning Books That Turn You from Novice to Expert | Data Drivenâ€¦</strong>
<em>The booming growth in the Machine Learning industry has brought renewed interest in people about Artificialâ€¦</em>www.datadriveninvestor.com</a></p>
<p>I picked threeÂ styles:</p>
<ul>
<li>
<p>Lishu(éš¶ä¹¦)</p>
</li>
<li>
<p>Kaishu(æ¥·ä¹¦)</p>
</li>
<li>
<p>Xiaozhuan(å°ç¯†)</p>
</li>
</ul>
<p>as a proof-of-concept. Once successful trained, the model could serve as a transfer learning base-model for the more fine-grained classifier( e.g. calligraphers classifier). This has some real-life value. From time to time, some ancient artifacts are discovered and some of them are calligraphy artworks. Sometimes itâ€™s hard to tell whose work it is. Is it valuable (like undiscovered artwork by a famousÂ calligrapher)?</p>
<blockquote>
<p>This calligrapher classifier can serve as a way to quickly identify artworks by great calligraphers. ( Finding diamond in the rough <em>ğŸ˜‰</em>)</p>
</blockquote>
<h3 id="collecting-data">Collecting Data<a class="headerlink" href="#collecting-data" title="Permanent link">Â¶</a></h3>
<p>To build a calligraphy classifier, we will need some calligraphy examples of each style. I did some search online and cannot find any good already-made data-set for different calligraphy styles. So Iâ€™ll have to build itÂ myself.</p>
<p>Building a images data-set isnâ€™t hard thanks to Googleâ€™s Images search functionality and some JavaScript snippets. Hereâ€™sÂ how:</p>
<ol>
<li>
<p>Go to <a href="https://www.google.com/imghp?hl=en">Google Images</a> and search for â€œéš¶ä¹¦ å­—å¸– ç½‘æ ¼â€ (lishu, characters book, grid), this will give you the most relevantÂ results.</p>
</li>
<li>
<p>Scroll down to show more results, youâ€™ll hit the bottom with â€˜<em>Show more results</em>â€™ button. Click if you want more, but keep in mind that <strong>700</strong> images is the maximumÂ here.</p>
</li>
</ol>
<p><img alt="Google search results for Lishu style" src="https://cdn-images-1.medium.com/max/2000/1*uQPNDb-qXO3mYQIHuxitMQ.png"/><em>Google search results for LishuÂ style</em></p>
<ol>
<li>Now is where the magic happens. Press Ctrl+Shift+J in Windows/Linux and Cmd+Opt+J in Mac to bring up the JavaScript â€˜Consoleâ€™ window of the browser. The following JavaScript snippet will get the URLs of each of theÂ images.</li>
</ol>
<p>4) If successfully run, a text file will be downloaded with all the URLs for the images in your search results. You can then set up a folder and use fast.aiâ€™s â€˜download_imagesâ€™ function to download theseÂ images.</p>
<p><img alt="" src="https://cdn-images-1.medium.com/max/2000/1*19mOhygnBZfGmX4S2fD4ww.png"/></p>
<ol>
<li>
<p>Rinse and repeat for other styles. You might want to put them into different folders like kaishu, xiaozhuan and put them all under a folder called train so later on, fast.ai can easily import them into theÂ model.</p>
</li>
<li>
<p>Alternatively, you can also go to Baidu.com for images search, using this <a href="https://gist.github.com/wayofnumbers/39842bb909c04070de49e53c418d512f">snippet</a> to automatically download the images you searched for. (Code too long to be put into this post, so I link itÂ here).</p>
</li>
</ol>
<h3 id="lets-have-a-look-at-the-data">Letâ€™s Have a Look at the Data<a class="headerlink" href="#lets-have-a-look-at-the-data" title="Permanent link">Â¶</a></h3>
<p>If you organize the downloaded images into train/lishu, train/kaishu, train/xiaozhuan, then you can run the following code to import them into and transformed accordingly, ready to fit a model.fast.aiâ€™s powerfulImageDataBunch object, where all data is organized, splitted and transformed accordingly, ready to fit aÂ model.</p>
<p><img alt="" src="https://cdn-images-1.medium.com/max/2000/1*hsF08GhTX9tjyov8hHzbFw.png"/></p>
<p>Note that we split the train/validation set with an 80/20 ratio, image resized to 224 which is a good default for any image recognitionÂ task.</p>
<p>Now that data is imported properly, letâ€™s look at ourÂ images:</p>
<p><img alt="" src="https://cdn-images-1.medium.com/max/2000/1*F378nVvqW7o6lz6QxAB2gA.png"/></p>
<p>As we can see from the few examples above, the data-set is rather â€˜dirtyâ€™. The images are not properly cropped, with some side notes with different calligraphy style and some images only have one or two characters. But itâ€™s <span class="caps">OK</span>. Letâ€™s quickly train the model and see how it performs so we can gain some insights into ourÂ data.</p>
<h3 id="quick-and-dirty-training">Quick and Dirty Training<a class="headerlink" href="#quick-and-dirty-training" title="Permanent link">Â¶</a></h3>
<p>Letâ€™s first create a model. Weâ€™ll be using transfer learning and use ResNet50 as our model. Pre-trained weights will beÂ downloaded.</p>
<p><img alt="" src="https://cdn-images-1.medium.com/max/2134/1*Zzy9-o-Q3K3BID_vZyRCoA.png"/></p>
<p>With 3 epoches of fit_one_cycle, we managed to reach a 90% accuracy rate on our validation set. NotÂ bad!</p>
<p><img alt="" src="https://cdn-images-1.medium.com/max/2000/1*44IMsadGzm0-mF2SdrvUKA.png"/></p>
<h3 id="unfreeze-and-fine-tune-our-training">Unfreeze and Fine-Tune Our Training<a class="headerlink" href="#unfreeze-and-fine-tune-our-training" title="Permanent link">Â¶</a></h3>
<p>Since the fit_one_cycle function will freeze the initial layers and only training the last couple of layers to speed up the training speed(this approach works because usually for transfer learning, initial layers will capture basic features of the images that are not likely to change a lot), we can hopefully further improve our accuracy by unfreezing all the layers and trainÂ again.</p>
<p><img alt="" src="https://cdn-images-1.medium.com/max/2000/1*IxzL6yxuHV2nqmoXure_fg.png"/></p>
<p>We used the above lr_find function to find a good learning rate range. The key is to find the steepest slope (as indicated in the orange circle above) in the learning curve and slice it for further training. For example, in the above figure, the bottom of the curve is at 1e-03, then we can pick one point at 1/10 of that, which is 1e-04, and the other one at 1e-06 or 1e-05 (This is inspired from an experimental concept of â€˜Super-convergenceâ€™, described in details in <a href="https://course.fast.ai">fast.ai course</a>. Sometime you need to do a bit of experiment to find the best learning rate combination but then again, fast.ai is always preaching iterative and interactive approach.) The idea is still to train the first couple of layers slower and last couple layersÂ faster.</p>
<p>Letâ€™s train another twoÂ epoch:</p>
<p><img alt="" src="https://cdn-images-1.medium.com/max/2000/1*noUlINi_AE8NZyZfJ7koBQ.png"/></p>
<p>Slightly better and the validation_loss starts to surpass train_loss, a sign of overfitting. Letâ€™s stop here and wrap thingsÂ up.</p>
<h3 id="results-interpretation">Results Interpretation<a class="headerlink" href="#results-interpretation" title="Permanent link">Â¶</a></h3>
<p>We reached 90% accuracy. Not state-of-the-art but already pretty impressive considering we only have a roughly 700 images per class data-set. More data will definitely lead to better results. Letâ€™s look at our results and see if we can find someÂ insights.</p>
<p><img alt="" src="https://cdn-images-1.medium.com/max/2000/1*tyskbmlwBIE0roKxHHbseA.png"/></p>
<p>Using the ClassificationIntepretation object from fast.ai, we can easily calculate the top_losses and see what theyÂ are:</p>
<p><img alt="" src="https://cdn-images-1.medium.com/max/2000/1*3bL7M8zSjT-PLGVTqp3hEg.png"/></p>
<p>Look at the confusion matrix, the model does really well in recognize â€˜xiaozhuanâ€™, probably due to its unique strokeÂ arrangements.</p>
<p><img alt="" src="https://cdn-images-1.medium.com/max/2000/1*qtb-Te_AElPaO3mym-9RVw.png"/></p>
<p><strong>A couple ofÂ insights:</strong></p>
<blockquote>
<p>We still have totally wrong images like the grid one (2nd one on 1st row)
If there are too few (1st row, 1st column) or too many (2nd row, 2nd column) characters, the model will struggle.
Some image shows â€˜in-betweenâ€™ kind of styles which the model also had a hard time classify. Which is totally normal, since even human will have a hard time telling which style it belongsÂ to.</p>
</blockquote>
<h3 id="final-thoughts">Final Thoughts<a class="headerlink" href="#final-thoughts" title="Permanent link">Â¶</a></h3>
<p>This experimental project actually works exceedingly well with fast.ai library. <a href="undefined">Jeremy Howard</a> said on the course and I quote here (not exactly word by word, but I hope I captured the gist of it.Â ğŸ™):</p>
<blockquote>
<h1 id="fastai-is-a-very-opinionated-library-wherever-we-know-a-best-default-well-choose-it-for-you-whatever-best-practice-we-know-works-well-well-do-it-for-you">fast.ai is a very opinionated library. Wherever we know a best default, weâ€™ll choose it for you. Whatever best practice we know works well, weâ€™ll do it for you.<a class="headerlink" href="#fastai-is-a-very-opinionated-library-wherever-we-know-a-best-default-well-choose-it-for-you-whatever-best-practice-we-know-works-well-well-do-it-for-you" title="Permanent link">Â¶</a></h1>
</blockquote>
<p>This is at least proven in this project. With only very few lines of code and very minimum efforts for data collection, we managed a 90% accurate model. I believe with more and better quality data. The state-of-the-art results could be achieved and our calligrapher classifier vision is not beyondÂ reach.</p>
<p><img alt="fast.aiâ€™s tagline: Making neural nets uncool again." src="https://cdn-images-1.medium.com/max/2400/0*Yo5w5gd2_CRC1MFl.jpg"/><em>fast.aiâ€™s tagline: Making neural nets uncoolÂ again.</em></p>
<p>Finally, allow me to paraphrase above tagline with a ChineseÂ poet:</p>
<p><img alt="â€œWhere once the swallows knew the mansions of the great, They now to humbler homes would fly to nest and mate.â€œ" src="https://cdn-images-1.medium.com/max/2000/1*g6k1Z7hyyeW_Y8Ge3GWmWQ.png"/><em>â€œWhere once the swallows knew the mansions of the great, They now to humbler homes would fly to nest andÂ mate.â€œ</em></p>
<p>You could find out how I fine-tuned the model and achieved better accuracy at the link below:
<a href="https://medium.com/datadriveninvestor/chinese-calligraphy-classifier-fine-tuning-cbfbf0e304d8"><strong>How I Trained Computer to Learn Calligraphy Styles: Part 2</strong>
<em>Build a Deep Learning Models with fast.ai Library</em>medium.com</a></p>
  </div><!-- /.entry-content -->
</section>
        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="http://getpelican.com/">Pelican</a>,
                which takes great advantage of <a href="http://python.org">Python</a>.
                </address><!-- /#about -->
        </footer><!-- /#contentinfo -->
</body>
</html>